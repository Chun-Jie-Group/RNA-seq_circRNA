# This line is required. It pulls in default overrides from the embedded cromwell `application.conf` needed for proper
# performance of cromwell.
include required(classpath("application"))

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}

backend {
  # Override the default backend.
  default = "Local"

  # The list of providers.
  providers {

    # The local provider is included by default in the reference.conf. This is an example.

    # Define a new backend provider.
    Local {
      # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend.
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      # The backend custom configuration.
      config {

        # Optional limits on the number of concurrent jobs
        concurrent-job-limit =15

        # If true submits scripts to the bash background using "&". Only usefull for dispatchers that do NOT submit
        # the job and then immediately return a scheduled job id.
        run-in-background = true

        # `temporary-directory` creates the temporary directory for commands.
        #
        # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:
        # temporary-directory = "$(mktemp -d \"$PWD\"/tmp.XXXXXX)"
        #
        # The expression is run from the execution directory for the script. The expression must create the directory
        # if it does not exist, and then return the full path to the directory.
        #
        # To create and return a non-random temporary directory, use something like:
        # temporary-directory = "$(mkdir -p /tmp/mydir && echo /tmp/mydir)"

        # `script-epilogue` configures a shell command to run after the execution of every command block.
        #
        # If this value is not set explicitly, the default value is `sync`, equivalent to:
        # script-epilogue = "sync"
        #
        # To turn off the default `sync` behavior set this value to an empty string:
        # script-epilogue = ""

        # The list of possible runtime custom attributes.
        #runtime-attributes = """
        #String? docker
        #String docker_user = "$EUID:root" 
        #"""

        # Submit string when there is no "docker" runtime attribute.
        submit = "/usr/bin/env bash ${script}"

        # Submit string when there is a "docker" runtime attribute.
        #submit-docker = """
        #docker run \
         # --rm -i \
          #${"--user " + docker_user} \
          #--entrypoint ${job_shell} \
          #-v ${cwd}:${docker_cwd} \
          #${docker} ${script}
        #"""

        # Root directory where Cromwell writes job results.  This directory must be
        # visible and writeable by the Cromwell process as well as the jobs that Cromwell
        # launches.
        root = "cromwell-executions"

        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above.
        #dockerRoot = "/cromwell-executions"

        # File system configuration.
        filesystems {

          # For SFS backends, the "local" configuration specifies how files are handled.
          local {

            # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
            localization: [
              "hard-link"
            ]

            # Call caching strategies
            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "file"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            }
          }
        }

        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          failOnStderr: false
          continueOnReturnCode: 0
        }
      }
    }

    # Other backend examples. Uncomment the block/stanza for your configuration. May need more tweaking/configuration
    # for your specific environment.

    #TES {
    #  actor-factory = "cromwell.backend.impl.tes.TesBackendLifecycleActorFactory"
    #  config {
    #    root = "cromwell-executions"
    #    dockerRoot = "/cromwell-executions"
    #    endpoint = "http://127.0.0.1:9000/v1/tasks"
    #    default-runtime-attributes {
    #      cpu: 1
    #      failOnStderr: false
    #      continueOnReturnCode: 0
    #      memory: "2 GB"
    #      disk: "2 GB"
    #    }
    #  }
    #}

    #TESK {
    #  actor-factory = "cromwell.backend.impl.tes.TesBackendLifecycleActorFactory"
    #  config {
    #    root = "ftp://your.ftphost.com/cromwell-executions"
    #    dockerRoot = "/cromwell-executions"
    #    endpoint = "http://your-tesk-endpoint/v1/tasks"
    #    glob-link-command = "ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY"
    #  }
    #}

    #BCS {
    #  actor-factory = "cromwell.backend.impl.bcs.BcsBackendLifecycleActorFactory"
    #  config {
    #    root = "oss://your-bucket/cromwell-exe"
    #    dockerRoot = "/cromwell-executions"
    #    region = ""

    #    #access-id = ""
    #    #access-key = ""
	#	#security-token = ""

    #    filesystems {
    #      oss {
    #        auth {
    #            #endpoint = ""
    #            #access-id = ""
    #            #access-key = ""
	#	        #security-token = ""
    #        }
    #      }
    #    }

    #    default-runtime-attributes {
    #        #failOnStderr: false
    #        #continueOnReturnCode: 0
    #        #cluster: "cls-mycluster"
    #        #mounts: "oss://bcs-bucket/bcs-dir/ /home/inputs/ false"
    #        #docker: "ubuntu/latest oss://bcs-reg/ubuntu/"
    #        #userData: "key value"
    #        #reserveOnFail: true
    #        #autoReleaseJob: true
    #        #verbose: false
    #        #workerPath: "oss://bcs-bucket/workflow/worker.tar.gz"
    #        #systemDisk: "cloud 50"
    #        #dataDisk: "cloud 250 /home/data/"
    #        #timeout: 3000
    #        #vpc: "192.168.0.0/16 vpc-xxxx"
    #    }
    #  }
    #}

    #SGE {
    #  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    #  config {
    #
    #    # Limits the number of concurrent jobs
    #    concurrent-job-limit = 5
    #
    #    # If an 'exit-code-timeout-seconds' value is specified:
    #    # - When a job has not been alive for longer than this timeout
    #    # - And has still not produced an RC file
    #    # - Then it will be marked as Failed.
    #    # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout)
    #
    #    # exit-code-timeout-seconds = 120
    #
    #    runtime-attributes = """
    #    Int cpu = 1
    #    Float? memory_gb
    #    String? sge_queue
    #    String? sge_project
    #    """
    #
    #    submit = """
    #    qsub \
    #    -terse \
    #    -V \
    #    -b y \
    #    -N ${job_name} \
    #    -wd ${cwd} \
    #    -o ${out}.qsub \
    #    -e ${err}.qsub \
    #    -pe smp ${cpu} \
    #    ${"-l mem_free=" + memory_gb + "g"} \
    #    ${"-q " + sge_queue} \
    #    ${"-P " + sge_project} \
    #    /usr/bin/env bash ${script}
    #    """
    #
    #    kill = "qdel ${job_id}"
    #    check-alive = "qstat -j ${job_id}"
    #    job-id-regex = "(\\d+)"
    #  }
    #}

    #LSF {
    #  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    #  config {
    #    submit = "bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}"
    #    kill = "bkill ${job_id}"
    #    check-alive = "bjobs ${job_id}"
    #    job-id-regex = "Job <(\\d+)>.*"
    #  }
    #}

    #SLURM {
    #  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    #  config {
    #    runtime-attributes = """
    #    Int runtime_minutes = 600
    #    Int cpus = 2
    #    Int requested_memory_mb_per_core = 8000
    #    String queue = "short"
    #    """
    #
    #    # If an 'exit-code-timeout-seconds' value is specified:
    #    # - When a job has not been alive for longer than this timeout
    #    # - And has still not produced an RC file
    #    # - Then it will be marked as Failed.
    #    # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout)
    #
    #    # exit-code-timeout-seconds = 120
    #
    #    submit = """
    #        sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \
    #        ${"-n " + cpus} \
    #        --mem-per-cpu=${requested_memory_mb_per_core} \
    #        --wrap "/usr/bin/env bash ${script}"
    #    """
    #    kill = "scancel ${job_id}"
    #    check-alive = "squeue -j ${job_id}"
    #    job-id-regex = "Submitted batch job (\\d+).*"
    #  }
    #}

    # Example backend that _only_ runs workflows that specify docker for every command.
    #Docker {
    #  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    #  config {
    #    run-in-background = true
    #    runtime-attributes = "String docker"
    #    submit-docker = "docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"
    #  }
    #}

    #HtCondor {
    #  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    #  config {
    #    runtime-attributes = """
    #      Int cpu = 1
    #      Float memory_mb = 512.0
    #      Float disk_kb = 256000.0
    #      String? nativeSpecs
    #      String? docker
    #    """
    #
    #    # If an 'exit-code-timeout-seconds' value is specified:
    #    # - When a job has not been alive for longer than this timeout
    #    # - And has still not produced an RC file
    #    # - Then it will be marked as Failed.
    #    # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout)
    #
    #    # exit-code-timeout-seconds = 120
    #
    #    submit = """
    #      chmod 755 ${script}
    #      cat > ${cwd}/execution/submitFile <<EOF
    #      Iwd=${cwd}/execution
    #      requirements=${nativeSpecs}
    #      leave_in_queue=true
    #      request_memory=${memory_mb}
    #      request_disk=${disk_kb}
    #      error=${err}
    #      output=${out}
    #      log_xml=true
    #      request_cpus=${cpu}
    #      executable=${script}
    #      log=${cwd}/execution/execution.log
    #      queue
    #      EOF
    #      condor_submit ${cwd}/execution/submitFile
    #    """
    #
    #    submit-docker = """
    #      chmod 755 ${script}
    #      cat > ${cwd}/execution/dockerScript <<EOF
    #      #!/bin/bash
    #      docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${script}
    #      EOF
    #      chmod 755 ${cwd}/execution/dockerScript
    #      cat > ${cwd}/execution/submitFile <<EOF
    #      Iwd=${cwd}/execution
    #      requirements=${nativeSpecs}
    #      leave_in_queue=true
    #      request_memory=${memory_mb}
    #      request_disk=${disk_kb}
    #      error=${cwd}/execution/stderr
    #      output=${cwd}/execution/stdout
    #      log_xml=true
    #      request_cpus=${cpu}
    #      executable=${cwd}/execution/dockerScript
    #      log=${cwd}/execution/execution.log
    #      queue
    #      EOF
    #      condor_submit ${cwd}/execution/submitFile
    #    """
    #
    #    kill = "condor_rm ${job_id}"
    #    check-alive = "condor_q ${job_id}"
    #    job-id-regex = "(?sm).*cluster (\\d+)..*"
    #  }
    #}

    #Spark {
    # actor-factory = "cromwell.backend.impl.spark.SparkBackendFactory"
    # config {
    #   # Root directory where Cromwell writes job results.  This directory must be
    #    # visible and writeable by the Cromwell process as well as the jobs that Cromwell
    #   # launches.
    #   root: "cromwell-executions"
    #
    #   filesystems {
    #     local {
    #       localization: [
    #         "hard-link", "soft-link", "copy"
    #       ]
    #     }
    #    }
    #      # change (master, deployMode) to (yarn, client), (yarn, cluster)
    #      #  or (spark://hostname:port, cluster) for spark standalone cluster mode
    #     master: "local"
    #     deployMode: "client"
    #  }
    # }

    #PAPI {
    #  actor-factory = "cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"
    #  config {
    #    # Google project
    #    project = "my-cromwell-workflows"
    #
    #    # Base bucket for workflow executions
    #    root = "gs://my-cromwell-workflows-bucket"
    #
    #    # Set this to the lower of the two values "Queries per 100 seconds" and "Queries per 100 seconds per user" for
    #    # your project.
    #    #
    #    # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will
    #    # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.
    #    # 1000 is the default "Queries per 100 seconds per user", 50000 is the default "Queries per 100 seconds"
    #    # See https://cloud.google.com/genomics/quotas for more information
    #    genomics-api-queries-per-100-seconds = 1000
    #
    #    # Polling for completion backs-off gradually for slower-running jobs.
    #    # This is the maximum polling interval (in seconds):
    #    maximum-polling-interval = 600
    #
    #    # Optional Dockerhub Credentials. Can be used to access private docker images.
    #    dockerhub {
    #      # account = ""
    #      # token = ""
    #    }
    #
    #    # Number of workers to assaign to PAPI requests
    #    request-workers = 3
    #
    #    genomics {
    #      # A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
    #      # Pipelines and manipulate auth JSONs.
    #      auth = "application-default"
    #
    #
    #      // alternative service account to use on the launched compute instance
    #      // NOTE: If combined with service account authorization, both that serivce account and this service account
    #      // must be able to read and write to the 'root' GCS path
    #      compute-service-account = "default"
    #
    #      # Endpoint for APIs, no reason to change this unless directed by Google.
    #      endpoint-url = "https://genomics.googleapis.com/"
    #
    #      # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service
    #      # account not owned by the submitting user
    #      restrict-metadata-access = false
    #      
    #      # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted
    #      # There is no logic to determine if the error was transient or not, everything is retried upon failure
    #      # Defaults to 3
    #      localization-attempts = 3
    #    }
    #
    #    filesystems {
    #      gcs {
    #        # A reference to a potentially different auth for manipulating files via engine functions.
    #        auth = "application-default"
    #        # Google project which will be billed for the requests
    #        project = "google-billing-project"
    #
    #        caching {
    #          # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs
    #          # Possible values: "copy", "reference". Defaults to "copy"
    #          # "copy": Copy the output files
    #          # "reference": DO NOT copy the output files but point to the original output files instead.
    #          #              Will still make sure than all the original output files exist and are accessible before
    #          #              going forward with the cache hit.
    #          duplication-strategy = "copy"
    #        }
    #      }
    #    }
    #
    #    default-runtime-attributes {
    #      cpu: 1
    #      failOnStderr: false
    #      continueOnReturnCode: 0
    #      memory: "2048 MB"
    #      bootDiskSizeGb: 10
    #      # Allowed to be a String, or a list of Strings
    #      disks: "local-disk 10 SSD"
    #      noAddress: false
    #      preemptible: 0
    #      zones: ["us-central1-a", "us-central1-b"]
    #    }
    #  }
    #}

    #AWS {
    #  actor-factory = "cromwell.backend.impl.aws.AwsBackendActorFactory"
    #  config {
    #    ## These two settings are required to authenticate with the ECS service:
    #    accessKeyId = "..."
    #    secretKey = "..."
    #  }
    #}

  }
}

#database {
  # mysql example
  #driver = "slick.driver.MySQLDriver$"
 # profile = "slick.jdbc.MySQLProfile$"

  # see all possible parameters and default values here:
  # http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(String,Config,Driver):Database
  #db {
   #driver = "com.mysql.jdbc.Driver"
   #url = "jdbc:mysql://localhost/cromwell?useSSL=false&rewriteBatchedStatements=true"
   #user = "cromwell"
  # password = "cromwell"
   #connectionTimeout = 5000
  #}

  # For batch inserts the number of inserts to send to the DB at a time
  # insert-batch-size = 2000

  #migration {
    # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of
    # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be
    # retrieved and processed at a time, before asking for the next chunk.
    #read-batch-size = 100000

    # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single
    # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out
    # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.
    #write-batch-size = 100000
  #}

  # To customize the metadata database connection, create a block under `database` with the metadata database settings.
  #
  # For example, the default database stores all data in memory. This commented out block would store `metadata` in an
  # hsqldb file, without modifying the internal engine database connection.
  #
  # The value `${uniqueSchema}` is always replaced with a unqiue UUID on each cromwell startup.
  #
  # This feature should be considered experimental and likely to change in the future.

  #metadata {
  #  profile = "slick.jdbc.HsqldbProfile$"
  #  db {
  #    driver = "org.hsqldb.jdbcDriver"
  #    url = "jdbc:hsqldb:file:metadata-${uniqueSchema};shutdown=false;hsqldb.tx=mvcc"
  #    connectionTimeout = 3000
  #  }
  #}
#}

# Cromwell "system" settings
system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  #abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  #graceful-server-shutdown = true

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  #workflow-restart = true

  # Cromwell will cap the number of running workflows at N
  #max-concurrent-workflows = 5000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  #max-workflow-launch-count = 50

  # Number of seconds between workflow launches
  #new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  #number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  #number-of-cache-read-workers = 25

  io {
    # Global Throttling - This is mostly useful for GCS and can be adjusted to match
    # the quota availble on the GCS API
    #number-of-requests = 100000
    #per = 100 seconds

    # Number of times an I/O operation should be attempted before giving up and failing it.
    #number-of-attempts = 5
  }

  # Maximum number of input file bytes allowed in order to read each type.
  # If exceeded a FileSizeTooBig exception will be thrown.
  input-read-limits {

    lines = 12800000

    #bool = 7

    #int = 19

    #float = 50

    #string = 128000

    #json = 128000

    #tsv = 128000

    #map = 128000

    #object = 128000
  }
}
